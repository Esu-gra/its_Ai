{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aaedddd",
   "metadata": {},
   "source": [
    "# Regressione lineare multivariata (prezzi immobili)\n",
    "\n",
    "Obiettivo: stimare il **prezzo** di un immobile a partire da alcune caratteristiche, tra cui la **superficie in m²**.\n",
    "\n",
    "Nel notebook vedrai:\n",
    "1. Un piccolo dataset di esempio (m², stanze, bagni, età)\n",
    "2. Normalizzazione delle feature\n",
    "3. Funzione di costo (MSE)\n",
    "4. Calcolo dei gradienti\n",
    "5. Gradient Descent passo-passo\n",
    "6. Predizione su nuovi immobili e interpretazione dei pesi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d028ed4",
   "metadata": {},
   "source": [
    "## 1) Dataset di esempio\n",
    "Ogni riga rappresenta un immobile.\n",
    "\n",
    "**Feature (X):**\n",
    "- `mq`: superficie in metri quadrati\n",
    "- `stanze`: numero di stanze\n",
    "- `bagni`: numero di bagni\n",
    "- `eta`: età dell'immobile in anni\n",
    "\n",
    "**Target (y):**\n",
    "- `prezzo`: prezzo in euro (qui in *migliaia di euro* per mantenere numeri più piccoli)\n",
    "\n",
    "> Nota: i dati sono fittizi ma realistici: servono per capire il metodo, non per fare stime di mercato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4081cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esempi (m) = 10, Feature (n) = 4\n",
      "Prima riga X_train: [55, 2, 1, 35] -> y: 150 k€\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1) DATI\n",
    "# =========================================================\n",
    "\n",
    "# X_train: lista di esempi, ogni esempio è una lista di feature [mq, stanze, bagni, eta]\n",
    "X_train = [\n",
    "    [55,  2, 1, 35],\n",
    "    [70,  3, 1, 20],\n",
    "    [85,  3, 2, 10],\n",
    "    [95,  4, 2,  8],\n",
    "    [110, 4, 2, 15],\n",
    "    [125, 5, 2,  5],\n",
    "    [140, 5, 3,  3],\n",
    "    [160, 6, 3, 12],\n",
    "    [180, 6, 3,  6],\n",
    "    [200, 7, 3,  2],\n",
    "]\n",
    "\n",
    "# y_train: prezzo in migliaia di euro (k€)\n",
    "y_train = [150, 190, 260, 295, 310, 380, 430, 450, 520, 600]\n",
    "\n",
    "m = len(X_train)      # numero di esempi\n",
    "n = len(X_train[0])   # numero di feature\n",
    "\n",
    "print(f\"Esempi (m) = {m}, Feature (n) = {n}\")\n",
    "print(\"Prima riga X_train:\", X_train[0], \"-> y:\", y_train[0], \"k€\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8514f",
   "metadata": {},
   "source": [
    "## 2) Modello\n",
    "Useremo il modello lineare multivariato:\n",
    "\n",
    "y_hat(i) = w1 * x1(i) + w2 * x2(i) + ... + wn * xn(i) + b\n",
    "\n",
    "\n",
    "dove:\n",
    "x1, x2, ..., xn sono le feature dell'immobile\n",
    "(esempio: metri quadrati, numero di stanze, numero di bagni, età)\n",
    "\n",
    "w1, w2, ..., wn sono i pesi del modello\n",
    "indicano quanto ogni feature influenza il prezzo\n",
    "\n",
    "b è l'intercetta (bias)\n",
    "serve a spostare la previsione verso l'alto o verso il basso\n",
    "\n",
    "y_hat è il prezzo stimato dal modello\n",
    "\n",
    "Implementiamo prima le funzioni di utilità: prodotto scalare, predizione e costo MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbc2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo iniziale J(w=0,b=0) = 73576.25\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2) FUNZIONI DI BASE (no numpy)\n",
    "# =========================================================\n",
    "import math\n",
    "\n",
    "def dot(a, b):\n",
    "    \"\"\"Prodotto scalare tra due liste di numeri della stessa lunghezza.\"\"\"\n",
    "    return sum(ai * bi for ai, bi in zip(a, b))\n",
    "\n",
    "def predict_one(x, w, b):\n",
    "    \"\"\"Predizione per un singolo esempio x.\"\"\"\n",
    "    return dot(w, x) + b\n",
    "\n",
    "def compute_cost_mse(X, y, w, b):\n",
    "    \"\"\"Costo MSE con fattore 1/(2m): J(w,b) = (1/(2m)) * sum((yhat - y)^2).\"\"\"\n",
    "    m = len(X)\n",
    "    total = 0.0\n",
    "    for xi, yi in zip(X, y):\n",
    "        yhat = predict_one(xi, w, b)\n",
    "        err = yhat - yi\n",
    "        total += err * err\n",
    "    return total / (2 * m)\n",
    "\n",
    "# inizializziamo w e b a zero\n",
    "w = [0.0] * n\n",
    "b = 0.0\n",
    "\n",
    "J0 = compute_cost_mse(X_train, y_train, w, b)\n",
    "print(\"Costo iniziale J(w=0,b=0) =\", J0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac279412",
   "metadata": {},
   "source": [
    "## 3) Perché normalizzare le feature\n",
    "\n",
    "Le feature hanno scale molto diverse tra loro.\n",
    "Ad esempio:\n",
    "mq può andare da circa 50 a 200\n",
    "stanze può andare da circa 2 a 7\n",
    "eta può andare da circa 2 a 35\n",
    "Quindi i valori hanno ordini di grandezza diversi.\n",
    "\n",
    "Problema senza normalizzazione\n",
    "Se lasciamo i valori così, il Gradient Descent può comportarsi male:\n",
    "fa passi troppo grandi su alcune feature fa passi troppo piccoli su altre converge lentamente oppure può oscillare e non convergere bene\n",
    "Questo rende l'apprendimento inefficiente.\n",
    "\n",
    "Soluzione: normalizzazione con z-score\n",
    "\n",
    "Trasformiamo ogni feature usando la formula:\n",
    "x_norm = (x - mu) / sigma\n",
    "dove:\n",
    "x = valore originale della feature\n",
    "mu = media della feature\n",
    "sigma = deviazione standard della feature\n",
    "x_norm = valore normalizzato\n",
    "\n",
    "Cosa fa questa trasformazione\n",
    "\n",
    "Dopo la normalizzazione:\n",
    "la media diventa circa 0\n",
    "la deviazione standard diventa circa 1\n",
    "tutte le feature hanno scale simili\n",
    "\n",
    "Questo rende il Gradient Descent molto più efficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d480e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media (mu) per feature: [122, 4.5, 2.2, 11.6]\n",
      "Dev std (sigma) per feature: [45.45327270945405, 1.5, 0.7483314773547883, 9.414881836751857]\n",
      "Esempio normalizzato (prima riga):\n",
      "  X originale: [55, 2, 1, 35]\n",
      "  X norm.:    [-1.474, -1.667, -1.604, 2.485]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3) NORMALIZZAZIONE (z-score) - solo standard library\n",
    "# =========================================================\n",
    "import statistics\n",
    "\n",
    "def column_stats(X):\n",
    "    \"\"\"Ritorna (mu_list, sigma_list) per ciascuna colonna (feature).\"\"\"\n",
    "    n = len(X[0])\n",
    "    mus = []\n",
    "    sigmas = []\n",
    "    for j in range(n):\n",
    "        col = [row[j] for row in X]\n",
    "        mu = statistics.mean(col)\n",
    "        # pstdev = deviazione standard \"population\" (divisione per m), stabile per ML didattico\n",
    "        sigma = statistics.pstdev(col)\n",
    "        # evitiamo divisione per zero nel caso sigma=0\n",
    "        sigma = sigma if sigma != 0 else 1.0\n",
    "        mus.append(mu)\n",
    "        sigmas.append(sigma)\n",
    "    return mus, sigmas\n",
    "\n",
    "def normalize_features(X, mus, sigmas):\n",
    "    \"\"\"Applica z-score a tutto X.\"\"\"\n",
    "    Xn = []\n",
    "    for row in X:\n",
    "        Xn.append([(row[j] - mus[j]) / sigmas[j] for j in range(len(row))])\n",
    "    return Xn\n",
    "\n",
    "mus, sigmas = column_stats(X_train)\n",
    "Xn_train = normalize_features(X_train, mus, sigmas)\n",
    "\n",
    "print(\"Media (mu) per feature:\", mus)\n",
    "print(\"Dev std (sigma) per feature:\", sigmas)\n",
    "print(\"Esempio normalizzato (prima riga):\")\n",
    "print(\"  X originale:\", X_train[0])\n",
    "print(\"  X norm.:   \", [round(v, 3) for v in Xn_train[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76c2f9",
   "metadata": {},
   "source": [
    "## 4) Gradient Descent: idea e formule\n",
    "\n",
    "Vogliamo trovare i valori migliori dei pesi w e dell'intercetta b che minimizzano la funzione di costo.\n",
    "Ricorda che la funzione di costo è:\n",
    "J(w,b) = (1/(2m)) * somma( (y_hat - y)^2 )\n",
    "\n",
    "Per la regressione lineare, questa funzione ha una forma a \"ciotola\" (convessa), quindi esiste un solo minimo globale.\n",
    "Il Gradient Descent è un algoritmo che trova questo minimo facendo piccoli passi nella direzione che riduce il costo.\n",
    "\n",
    "4.1 Errore per ogni esempio\n",
    "Per ogni immobile i:\n",
    "errore(i) = y_hat(i) - y(i)\n",
    "dove:\n",
    "y_hat(i) = prezzo stimato\n",
    "y(i) = prezzo reale\n",
    "\n",
    "4.2 Gradiente rispetto ai pesi\n",
    "Per ogni peso wj:\n",
    "derivata_wj = (1/m) * somma( errore(i) * xj(i) )\n",
    "dove:\n",
    "m = numero di esempi\n",
    "xj(i) = valore della feature j per l'esempio i\n",
    "Questo valore indica quanto modificare il peso wj.\n",
    "\n",
    "4.3 Gradiente rispetto all'intercetta\n",
    "Per l'intercetta b:\n",
    "derivata_b = (1/m) * somma( errore(i) )\n",
    "Questo indica quanto modificare b.\n",
    "\n",
    "4.4 Aggiornamento dei parametri\n",
    "Aggiorniamo i parametri usando il tasso di apprendimento alpha:\n",
    "wj = wj - alpha * derivata_wj\n",
    "b = b - alpha * derivata_b\n",
    "\n",
    "4.5 Significato intuitivo\n",
    "Il Gradient Descent funziona così:\n",
    "calcola l'errore\n",
    "calcola in quale direzione muoversi\n",
    "fa un piccolo passo in quella direzione\n",
    "ripete il processo molte volte\n",
    "Ad ogni iterazione il costo diminuisce.\n",
    "\n",
    "4.6 Intuizione geometrica\n",
    "Immagina di essere su una montagna:\n",
    "il costo J è l'altezza\n",
    "vuoi raggiungere il punto più basso\n",
    "il gradiente indica la direzione di salita\n",
    "andando nella direzione opposta, scendi verso il minimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877442e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter    1 | J = 56135.0143 | b =  35.8500 | w = [13.5755, 13.3833, 12.6014, -10.6385]\n",
      "Iter  200 | J =    85.6881 | b = 358.5000 | w = [72.9834, 43.0447, 14.5528, -8.6832]\n",
      "Iter  400 | J =    68.1718 | b = 358.5000 | w = [86.9988, 32.0191, 9.6429, -10.7911]\n",
      "Iter  600 | J =    57.6475 | b = 358.5000 | w = [97.6228, 22.6147, 7.1461, -12.2095]\n",
      "Iter  800 | J =    51.1856 | b = 358.5000 | w = [105.9166, 15.1743, 5.3154, -13.2968]\n",
      "Iter 1000 | J =    47.2167 | b = 358.5000 | w = [112.4136, 9.3367, 3.8923, -14.1466]\n",
      "Iter 1200 | J =    44.7790 | b = 358.5000 | w = [117.505, 4.7611, 2.7782, -14.8124]\n",
      "Iter 1400 | J =    43.2818 | b = 358.5000 | w = [121.4951, 1.1751, 1.9051, -15.3342]\n",
      "Iter 1600 | J =    42.3622 | b = 358.5000 | w = [124.6222, -1.6353, 1.2209, -15.7431]\n",
      "Iter 1800 | J =    41.7974 | b = 358.5000 | w = [127.073, -3.8378, 0.6846, -16.0636]\n",
      "Iter 2000 | J =    41.4505 | b = 358.5000 | w = [128.9936, -5.564, 0.2644, -16.3147]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 4) GRADIENTI e GRADIENT DESCENT (no numpy)\n",
    "# =========================================================\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"Calcola dj/dw e dj/db (gradiente della funzione di costo).\"\"\"\n",
    "    m = len(X)\n",
    "    n = len(X[0])\n",
    "    dj_dw = [0.0] * n\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for xi, yi in zip(X, y):\n",
    "        yhat = predict_one(xi, w, b)\n",
    "        err = yhat - yi\n",
    "\n",
    "        # accumula per b\n",
    "        dj_db += err\n",
    "\n",
    "        # accumula per ogni w_j\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * xi[j]\n",
    "\n",
    "    # media\n",
    "    dj_db /= m\n",
    "    dj_dw = [v / m for v in dj_dw]\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iters, print_every=100):\n",
    "    \"\"\"Esegue Gradient Descent e ritorna (w, b, history).\"\"\"\n",
    "    w = w_init[:]\n",
    "    b = b_init\n",
    "    history = []\n",
    "\n",
    "    for it in range(1, num_iters + 1):\n",
    "        dj_dw, dj_db = compute_gradients(X, y, w, b)\n",
    "\n",
    "        # aggiornamento simultaneo\n",
    "        for j in range(len(w)):\n",
    "            w[j] -= alpha * dj_dw[j]\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        # log del costo\n",
    "        if it == 1 or it % print_every == 0 or it == num_iters:\n",
    "            J = compute_cost_mse(X, y, w, b)\n",
    "            history.append((it, J))\n",
    "            print(f\"Iter {it:4d} | J = {J:10.4f} | b = {b:8.4f} | w = {[round(v,4) for v in w]}\")\n",
    "    return w, b, history\n",
    "\n",
    "# Iperparametri\n",
    "alpha = 0.1\n",
    "num_iters = 2000\n",
    "\n",
    "# attenzione: usiamo X normalizzato!\n",
    "w0 = [0.0] * n\n",
    "b0 = 0.0\n",
    "\n",
    "w_trained, b_trained, hist = gradient_descent(Xn_train, y_train, w0, b0, alpha, num_iters, print_every=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bea4a",
   "metadata": {},
   "source": [
    "## 5) Interpretare il risultato\n",
    "Abbiamo allenato il modello su feature **normalizzate**.\n",
    "\n",
    "- Se un peso \\(w_j\\) è positivo, aumentando quella feature (in termini di deviazioni standard) il prezzo stimato aumenta.\n",
    "- Se è negativo, l'effetto è opposto.\n",
    "\n",
    "Attenzione però: con normalizzazione, i pesi non sono \"€/m²\" direttamente; sono \"k€ per 1 deviazione standard\" della feature.\n",
    "\n",
    "Per fare predizioni su nuovi dati, dobbiamo:\n",
    "1) prendere le feature grezze\n",
    "2) normalizzarle con \\(\\mu\\) e \\(\\sigma\\) del training set\n",
    "3) usare \\(w\\) e \\(b\\) allenati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7f33ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuovo immobile (raw): [100, 4, 2, 12]\n",
      "Prezzo stimato: 299.3 k€  (~ 299337 € )\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 5) PREDIZIONE SU NUOVI IMMOBILI\n",
    "# =========================================================\n",
    "\n",
    "def normalize_one(x_raw, mus, sigmas):\n",
    "    \"\"\"Normalizza un singolo esempio x_raw usando mus e sigmas del training.\"\"\"\n",
    "    return [(x_raw[j] - mus[j]) / sigmas[j] for j in range(len(x_raw))]\n",
    "\n",
    "def predict_price_k_euro(x_raw, w, b, mus, sigmas):\n",
    "    \"\"\"Ritorna la predizione in k€ per un immobile (feature grezze).\"\"\"\n",
    "    x_norm = normalize_one(x_raw, mus, sigmas)\n",
    "    return predict_one(x_norm, w, b)\n",
    "\n",
    "# Esempio: immobile 100 m², 4 stanze, 2 bagni, 12 anni\n",
    "x_new = [100, 4, 2, 12]\n",
    "pred_k = predict_price_k_euro(x_new, w_trained, b_trained, mus, sigmas)\n",
    "\n",
    "print(\"Nuovo immobile (raw):\", x_new)\n",
    "print(\"Prezzo stimato:\", round(pred_k, 1), \"k€  (~\", round(pred_k*1000), \"€ )\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc5de9",
   "metadata": {},
   "source": [
    "## 6) Micro-demo numerica: 10 passi di discesa (per vedere il percorso)\n",
    "Qui facciamo una mini-simulazione con **solo 10 iterazioni** e stampiamo:\n",
    "- costo \\(J\\)\n",
    "- valori di \\(b\\)\n",
    "- primi due pesi \\(w\\) (giusto per tenere la stampa leggibile)\n",
    "\n",
    "Questo aiuta gli studenti a vedere che il costo tende a scendere e che il minimo (per regressione lineare con MSE) è unico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bee30fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step |   J(w,b)   |    b    |   w[0] (mq)  |   w[1] (stanze) \n",
      "   1 | 56135.0143 |  35.850 |       13.575 |        13.383\n",
      "   2 | 44054.2856 |  68.115 |       22.550 |        22.158\n",
      "   3 | 35122.1149 |  97.154 |       28.547 |        27.951\n",
      "   4 | 28242.2406 | 123.288 |       32.615 |        31.815\n",
      "   5 | 22816.9229 | 146.809 |       35.433 |        34.427\n",
      "   6 | 18483.1189 | 167.978 |       37.438 |        36.228\n",
      "   7 | 14997.3131 | 187.031 |       38.913 |        37.501\n",
      "   8 | 12183.3523 | 204.178 |       40.042 |        38.429\n",
      "   9 |  9907.3530 | 219.610 |       40.943 |        39.131\n",
      "  10 |  8064.5404 | 233.499 |       41.692 |        39.683\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 6) MINI-DEMO: 10 step di gradient descent\n",
    "# =========================================================\n",
    "\n",
    "w_demo = [0.0] * n\n",
    "b_demo = 0.0\n",
    "alpha_demo = 0.1\n",
    "\n",
    "print(\"Step |   J(w,b)   |    b    |   w[0] (mq)  |   w[1] (stanze) \")\n",
    "for step in range(1, 11):\n",
    "    dj_dw, dj_db = compute_gradients(Xn_train, y_train, w_demo, b_demo)\n",
    "    for j in range(n):\n",
    "        w_demo[j] -= alpha_demo * dj_dw[j]\n",
    "    b_demo -= alpha_demo * dj_db\n",
    "\n",
    "    J = compute_cost_mse(Xn_train, y_train, w_demo, b_demo)\n",
    "    print(f\"{step:>4d} | {J:>10.4f} | {b_demo:>7.3f} | {w_demo[0]:>12.3f} | {w_demo[1]:>13.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234df320",
   "metadata": {},
   "source": [
    "## 7) Esercizio per studenti\n",
    "1) Aggiungi o modifica 2–3 righe nel dataset (nuovi immobili) e ri-esegui l'allenamento.\n",
    "2) Prova a cambiare \\(\\alpha\\) (es. 0.01, 0.3) e osserva:\n",
    "   - converge più lentamente?\n",
    "   - diverge (costo che esplode)?\n",
    "3) Prova a togliere la normalizzazione e confronta la stabilità.\n",
    "\n",
    "Spunto di discussione:\n",
    "- Quale feature sembra più influente sul prezzo? (guardando segno e grandezza dei pesi)\n",
    "- Perché l'età potrebbe avere un peso negativo?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
