{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4031fd8-ec0c-4e93-9feb-54d71c730d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Notebook: Preprocessing e Preparazione Dati per il Machine Learning\n",
    "    #**Obiettivo:** Utilizzare il dataset di feature estratto da pgAdmin \n",
    "    #(`customer_features.csv`) per creare una pipeline di preprocessing e \n",
    "    #preparare i dati per l'addestramento di un modello di Machine Learning. \n",
    "    #**Questo notebook si ferma prima del training del modello.**\"\n",
    "  \n",
    "    \n",
    "    ### 1. Import delle Librerie\"\n",
    " \n",
    "    import pandas as pd\n",
    "    import numpy as n\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from datetime import datetime\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909ac8a2-727d-463c-87d4-624ea16f0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRORE: File 'customer_features.csv' non trovato. Assicurati di averlo esportato da pgAdmin e salvato nella stessa directory di questo notebook.\n"
     ]
    }
   ],
   "source": [
    "                        \n",
    "    ### 2. Caricamento del Dataset\n",
    "    #\"Carichiamo il file `customer_features.csv` che hai esportato da pgAdmin.\"\n",
    "try:\n",
    "     df = pd.read_csv(\"customer_features.csv\")\n",
    "     print(\"Dataset caricato con successo!\")\n",
    "     display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"ERRORE: File 'customer_features.csv' non trovato. Assicurati di averlo esportato da pgAdmin e salvato nella stessa directory di questo notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f275c42-bfc5-4579-bd0e-3e9508fbe913",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Feature Engineering Aggiuntivo e Definizione del Target\\n\",\n",
    "#\"Creiamo le ultime feature derivate e la nostra variabile target `is_churn`.\"\n",
    "# Conversione delle colonne di data\"\n",
    "df[\"data_ultimo_ordine\"] = pd.to_datetime(df[\"data_ultimo_ordine\"])\n",
    "df[\"data_primo_ordine\"] = pd.to_datetime(df[\"data_primo_ordine\"])\n",
    "\n",
    "# Data di riferimento per il calcolo (es. il giorno dopo l'ultimo ordine registrato nel dataset)\n",
    "snapshot_date = df[\"data_ultimo_ordine\"].max() + pd.DateOffset(days=1)\n",
    "\n",
    "# Creazione feature di Recency e Tenure\n",
    "df[\"recency\"] = (snapshot_date - df[\"data_ultimo_ordine\"]).dt.days\n",
    "df[\"tenure\"] = (snapshot_date - df[\"data_primo_ordine\"]).dt.days\n",
    " \n",
    "# Definizione della variabile Target\n",
    "# Un cliente è considerato 'churned' se non ha fatto acquisti da più di 90 giorni\n",
    "df[\"is_churn\"] = (df[\"recency\"] > 90).astype(int)\n",
    "\n",
    "print(\"Distribuzione della variabile target:\")\n",
    "print(df[\"is_churn\"].value_counts(normalize=True))\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c4dc5-3170-4024-825c-42afeb2d4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Selezione delle Feature e Split Train/Test\n",
    "# Selezioniamo le colonne che useremo come feature (X) e la nostra variabile target (y). \n",
    "# Poi, dividiamo il dataset.\n",
    "# Selezioniamo le feature numeriche e categoriche\n",
    "numeric_features = [\"recency\", \"tenure\", \"numero_ordini\", \"fatturato_totale\", \"scontrino_medio\", \"totale_prodotti_acquistati\", \"num_prodotti_distinti\", \"num_categorie_distinte\"]\n",
    "categorical_features = [\"regione\"]\n",
    "\n",
    "# Definiamo X e y\"\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df[\"is_churn\"]\n",
    "\n",
    "# Suddivisione in set di training e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Dimensioni X_train: {X_train.shape}\")\n",
    "print(f\"Dimensioni X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9149632-8765-4ac0-aa34-7ea48a202e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Creazione della Pipeline di Preprocessing\n",
    "#Costruiamo una pipeline con `ColumnTransformer` per applicare trasformazioni diverse alle colonne numeriche e categoriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e4a68-e558-4d2c-aebc-9b45a7f7167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline numeriche: imputazione + scaling\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline categoriche: imputazione + one-hot encoding\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ColumnTransformer: combina le due pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66f69e-dc45-46dc-9ae8-61e200535793",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Applicazione del Preprocessing\n",
    "# Applichiamo la pipeline di preprocessing ai dati di training e test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aaae9a-e221-495e-bf2c-95f9ba090d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Dimensioni X_train dopo il preprocessing: {X_train_processed.shape}\")\n",
    "print(f\"Dimensioni X_test dopo il preprocessing: {X_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9de2bb-78a5-4340-a1c9-0f6ad00a2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Conclusione\n",
    "# Abbiamo costruito una pipeline che preprocessa i dati e li prepara per un modello di ML.\n",
    "\n",
    "# I dati `X_train_processed`, `y_train`, `X_test_processed`, `y_test` sono ora pronti per essere usati nel training e nella valutazione del modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bdfb0-6d35-4247-81f7-96ba8d9b1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.0 Dati pronti per il Machine Learning\n",
    "\n",
    "# Osserviamo un’estrazione delle feature dopo il preprocessing.\n",
    "# Questi sono i dati che verranno realmente forniti al modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8514126-6e10-493b-9b41-371482e07224",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ml = pd.DataFrame(\n",
    "    X_train_processed.toarray() if hasattr(X_train_processed, \"toarray\") else X_train_processed\n",
    ")\n",
    "\n",
    "display(X_train_ml.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150572b-1c79-47e6-83d9-a5f5e469be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.1 Effetto dello scaling sulle feature numeriche\n",
    "\n",
    "#Confrontiamo la distribuzione delle feature numeriche prima e dopo lo scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cde12d-dd41-4372-b07d-647c8470d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[numeric_features].boxplot(rot=90, figsize=(12,5))\n",
    "plt.title(\"Feature numeriche PRIMA dello scaling\")\n",
    "plt.show()\n",
    "\n",
    "X_scaled = pd.DataFrame(\n",
    "    preprocessor.named_transformers_[\"num\"]\n",
    "    .fit_transform(X_train[numeric_features]),\n",
    "    columns=numeric_features\n",
    ")\n",
    "\n",
    "X_scaled.boxplot(rot=90, figsize=(12,5))\n",
    "plt.title(\"Feature numeriche DOPO lo scaling\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df947d55-09e0-4dcb-a997-7a83b1ecb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.2 Trasformazione delle variabili categoriche (One-Hot Encoding)\n",
    "\n",
    "# Osserviamo come la variabile `regione` viene trasformata in più colonne numeriche.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8136f2-35fe-48d0-aa85-c2ded0783dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = preprocessor.named_transformers_[\"cat\"][\"onehot\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "cat_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b10eb3-43b4-4562-bf2b-3d24abcc4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.3 Relazione tra una feature chiave e il churn\n",
    "\n",
    "# Analizziamo la relazione tra `recency` e la variabile target `is_churn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f65824f-797f-493f-805f-967129f128d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"is_churn\", y=\"recency\", data=df)\n",
    "plt.title(\"Recency vs Churn\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7abc69-88d4-4607-8ce2-17cb5ab9c5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
